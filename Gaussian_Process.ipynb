{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Gaussian_Process.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"JmMtwMbe40JN","colab_type":"text"},"cell_type":"markdown","source":["# Gaussian Process Regression"]},{"metadata":{"id":"mXFfsqJc421F","colab_type":"text"},"cell_type":"markdown","source":["Dataset : $(X_i, Y_i)_{1⩽i⩽n}$ where $X_i  \\in \\mathbb{R}^d$ and $Y_i \\in \\mathbb{R}$"]},{"metadata":{"id":"kyUcoy3phok9","colab_type":"text"},"cell_type":"markdown","source":["Regression problem : $\\forall i \\in \\{1, 2, ... n\\}, Y_i = f^*(X_i) + \\epsilon_i$  \n"]},{"metadata":{"id":"LIv0w-iwiu1W","colab_type":"text"},"cell_type":"markdown","source":["Let's write the matrix form of this : $Y = f^*(X) + \\epsilon$  \n","where $Y = [Y_1, Y_2, ... Y_n]^T \\in \\mathbb{R}^n$  \n","and \n","$X = \\begin{pmatrix}\n","          - & X_1 & - \\\\\n","          - & X_2 & - \\\\\n","            & ... \\\\\n","          - & X_n & -\n","         \\end{pmatrix} \\in \\mathbb{R}^{n . d}$  \n","and $\\epsilon = [\\epsilon_1, \\epsilon_2, ... \\epsilon_n]^T \\in \\mathbb{R}^n$"]},{"metadata":{"id":"a-iQFDtchi9b","colab_type":"text"},"cell_type":"markdown","source":["## Model"]},{"metadata":{"id":"tVO7SgFvgnBj","colab_type":"text"},"cell_type":"markdown","source":["Model : $y = f(x) + \\epsilon$ where $f(.) \\sim GP(0, k_{\\gamma})$ and $\\epsilon \\sim N(0, \\sigma ^2 I_n)$"]},{"metadata":{"id":"mBrBd8kcEpI1","colab_type":"text"},"cell_type":"markdown","source":["In general, the kernel used is the RBF kernel, defined as follow :  \n","$k_\\gamma (z, z') = \\exp(-\\frac{1}{2} (z - z') / l^2)$"]},{"metadata":{"id":"czDaG3sfhhcQ","colab_type":"text"},"cell_type":"markdown","source":["## Prediction"]},{"metadata":{"id":"oUViGi3LhNL3","colab_type":"text"},"cell_type":"markdown","source":["Gaussian Process Regression is a nonparametric model. We can do prediction directly by using the conditional gaussian distribution."]},{"metadata":{"id":"9EeSQM8TjFqw","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"cGH2ZqsXjGWz","colab_type":"text"},"cell_type":"markdown","source":["## Find the best kernel parameters"]},{"metadata":{"id":"rS_5nm9TjRxQ","colab_type":"text"},"cell_type":"markdown","source":["Let's apply the model to the dataset : $Y = f(X) + \\epsilon$  \n","where $Y \\in \\mathbb{R}^n$, $X  \\in \\mathbb{R}^{n . d}$ and $\\epsilon \\in \\mathbb{R}^n$"]},{"metadata":{"id":"yeZpj-IkjrYk","colab_type":"text"},"cell_type":"markdown","source":["Because f(.) is a Gaussian Process, it means that $f(X) \\sim N(0, K_\\gamma)$ where $K_\\gamma = (k_\\gamma (X_i, X_j)_{1≤i, j≤n})$"]},{"metadata":{"id":"EER1DowWkZUH","colab_type":"text"},"cell_type":"markdown","source":["Since $\\epsilon \\sim N(0, \\sigma ^2 I_n)$, it implies that $ (Y | X; \\gamma) \\sim N(0, K_\\gamma + \\sigma ^2 I_n)$"]},{"metadata":{"id":"tSLF6m8fkt27","colab_type":"text"},"cell_type":"markdown","source":["Let's write the probability distribution :  \n","$P(Y=y | X; \\gamma) = \\frac{1}{(2\\pi)^{n/2} det(K_\\gamma + \\sigma^2 I_n)^{1/2}}exp(-\\frac{1}{2} y^T (K_\\gamma + \\sigma^2 I_n)^{-1} y)$  "]},{"metadata":{"id":"c-7Qogrxk59t","colab_type":"text"},"cell_type":"markdown","source":["We define the likelihood of the model, given the observed data :  \n","$L(\\gamma) = P(Y=y | X; \\gamma)$  \n","We want to find the parameters $\\gamma$ which maximizes the likelihood function (Maximum Likelihood Estimation). Ordinarily, we prefer to maximize the log-likelihood : $\\log(L(\\gamma))$  \n","To unify this to the neural network architecture that we will use later, we will instead minimizes the negative log-likelihood :  \n","$l(\\gamma) = - \\log (P(Y=y | X; \\gamma))$"]},{"metadata":{"id":"d4A7aOScmI3d","colab_type":"text"},"cell_type":"markdown","source":["Let's write the resulting equation :  \n","$l(\\gamma) = \\frac{1}{2} ( n\\log(2\\pi) + \\log det(K_\\gamma + \\sigma^2 I_n) + y^T (K_\\gamma + \\sigma^2 I_n)^{-1} y )$  \n","To simplify the notation, let's say that $K_\\gamma = K_\\gamma + \\sigma ^2 I_n$"]},{"metadata":{"id":"6Som8Plcmuyr","colab_type":"text"},"cell_type":"markdown","source":["Again, to unify this with the neural network approach (based on the chain rule), we need to compute le derivative of the negative log-likelihood with respect to $K_\\gamma$ :  \n","$\\frac {\\partial l}{\\partial K_\\gamma} = \\frac{1}{2} ( \\frac {\\partial n\\log(2\\pi)}{\\partial K_\\gamma} + \\frac {\\partial \\log det K_\\gamma }{\\partial K_\\gamma} + \\frac {\\partial y^T K_\\gamma^{-1} y )}{\\partial K_\\gamma})$"]},{"metadata":{"id":"7OX6rOcfu_BY","colab_type":"text"},"cell_type":"markdown","source":["Pre-requisites :  \n","$\\frac {\\partial \\log \\det A}{\\partial A} = A^{-1}$  \n","$\\frac {\\partial A^{-1}}{\\partial A} = -(A^{-1}) (A^{-1})$"]},{"metadata":{"id":"a_m-5JbspgPn","colab_type":"text"},"cell_type":"markdown","source":["$\\frac {\\partial n\\log(2\\pi)}{\\partial K_\\gamma} = 0$  \n","$\\frac {\\partial \\log det K_\\gamma}{\\partial K_\\gamma} = K_\\gamma ^{-1}$  \n","$\\frac {\\partial y^T K_\\gamma^{-1} y}{\\partial K_\\gamma} = -y^T K_\\gamma^{-1} K_\\gamma^{-1} y$"]},{"metadata":{"id":"RcqrYPVtvmsA","colab_type":"text"},"cell_type":"markdown","source":["Finally :  \n","$\\frac {\\partial l}{\\partial K_\\gamma} = \\frac{1}{2} (K_\\gamma ^{-1} - y^T K_\\gamma^{-1} K_\\gamma^{-1} y)$  \n","In the \"Deep Kernel Learning\" paper, the final result is : $\\frac {\\partial l}{\\partial K_\\gamma} = \\frac{1}{2} (K_\\gamma ^{-1} - K_\\gamma^{-1} y y^T K_\\gamma^{-1})$"]},{"metadata":{"id":"nqLpimaiBgmG","colab_type":"text"},"cell_type":"markdown","source":["In the case of Deep Kernel Learning, we have $K_{\\gamma, w} = (k_\\gamma (h_w(X_i), h_w(X_j))_{1≤i, j≤n})$ where $h_w(.)$ represents the Neural Network.  \n","During the backpropagation, we use the chain rule to compute $\\frac {\\partial K_{\\gamma, w}}{\\partial \\gamma}$ and $\\frac {\\partial K_{\\gamma, w}}{\\partial w}$ in the following way :  \n","$\\frac {\\partial l}{\\partial \\gamma} = \\frac {\\partial l}{\\partial K_{\\gamma, w}} \\frac {\\partial K_{\\gamma, w}}{\\partial \\gamma}$  \n","$\\frac {\\partial l}{\\partial w} = \\frac {\\partial l}{\\partial K_{\\gamma, w}} \\frac {\\partial K_{\\gamma, w}}{\\partial w}$  "]},{"metadata":{"id":"RQn7bGWeepjn","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}