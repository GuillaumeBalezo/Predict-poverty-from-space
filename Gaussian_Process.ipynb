{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JmMtwMbe40JN"
   },
   "source": [
    "# Gaussian Process Regression\n",
    "\n",
    "\n",
    "________\n",
    "###  Table of content\n",
    "\n",
    "[1. Regression model](#model)<br>\n",
    "[2. Prediction](#Prediction)<br>\n",
    "[3. Finding the best kernel parameters](#kernel)<br>\n",
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mXFfsqJc421F"
   },
   "source": [
    "**Dataset :**\n",
    "\n",
    "Let $(X_i)_{i\\in[\\![1,n]\\!]}$ be i.i.d. random variables in $\\mathbb{R}^d$ and consider the matrix $X \\in \\mathbb{R}^{n . d}$ such that the i-th row of $X$ is the observation $X_i^T$. For all $1 \\leq i \\leq n$, $X_i$ is an individual which has been associated with the label $Y_i \\in \\mathbb{R}$, and consider the matrix $Y = [Y_1, Y_2, ..., Y_n]^T \\in \\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a-iQFDtchi9b"
   },
   "source": [
    "<a id='model'></a>\n",
    "## 1. Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Gaussian process regression model, $Y = f(X) + \\epsilon$. \n",
    "<br><br>\n",
    "The $(\\epsilon_i)_{i\\in[\\![1,n]\\!]} \\in \\mathbb{R}^n$ are the i.i.d. noise variables with independant normal distributions, so that $\\epsilon = [\\epsilon_1, \\epsilon_2, ... \\epsilon_n]^T \\sim N(0, \\sigma ^2 I_n)$.\n",
    "$ f$ is a Gaussian Process, i.e. an n-dimensional vector, defined by :\n",
    "\n",
    "\n",
    "$$\n",
    "f(.) \\sim \\mathcal{GP}(0, k_{\\gamma}(. , .))\n",
    "$$\n",
    "\n",
    "where $k_{\\gamma}(. , .)$ is a valid covariance function and $\\gamma$ is the parameter to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mBrBd8kcEpI1"
   },
   "source": [
    "A commonly used kernel function is the squared exponential or radial basis function (RBF) kernel, defined as follows:\n",
    "\n",
    "$$k_\\gamma (z, z') = \\exp(-\\frac{\\parallel z - z' \\parallel ^2}{2 \\gamma^2})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "czDaG3sfhhcQ"
   },
   "source": [
    "<a id='prediction'></a>\n",
    "## 2. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oUViGi3LhNL3"
   },
   "source": [
    "Gaussian Process Regression is a nonparametric model. Therein, prediction will be directly performed using the conditionnal gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9EeSQM8TjFqw"
   },
   "source": [
    "Given a dataset of observed outputs $ \\lbrace (x_i, y_i) \\rbrace_{1 \\leq i \\leq n}$, we want to predict the output $Y_{test}$ of a test set $X_{test}$ drawn from the same distribution.\n",
    "\n",
    "\n",
    "With $Y_0 = [Y_{test}, Y]^T$, $X_0 = [X_{test}, X]^T$ and $\\epsilon_0 = [0, \\epsilon]^T$, the model is thus :\n",
    "\n",
    "\n",
    "$$Y_0 = f(X_0) + \\epsilon_0 \\sim N(0, K_0 + \\sigma^2I_n)$$\n",
    "\n",
    "\n",
    "where \n",
    "$$K_0 = \\begin{pmatrix}\n",
    "          K_{aa} & K_{ab} \\\\\n",
    "          K_{ba} & K_{bb} \\\\\n",
    "         \\end{pmatrix}$$ and \n",
    "$$\\begin{equation}\n",
    "    \\begin{cases}\n",
    "      K_{aa} = (k_\\gamma(X_i, X_j))\\\\\n",
    "      K_{ab} = (k_\\gamma(X_i, X_{test, j}))\\\\\n",
    "      K_{ba} = (k_\\gamma(X_{test, i}, X_j))\\\\\n",
    "      K_{bb} = (k_\\gamma(X_{test, i}, X_{test, j}))\n",
    "    \\end{cases}       \n",
    "\\end{equation}$$\n",
    "\n",
    "We can then compute the conditional distribution $(Y_{test}|X_{test}, X, Y) \\sim \\mathcal{N}(m, D)$ by using the conditional Gaussian distribution formulas :\n",
    "\n",
    "$$m = K_{ab}K_{bb}^{-1}$$\n",
    "$$D = (K_{aa} + \\sigma^2I_n) - K_{ab}K_{bb}^{-1}K_{ba}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cGH2ZqsXjGWz"
   },
   "source": [
    "<a id='kernel'></a>\n",
    "## 3. Finding the best kernel parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rS_5nm9TjRxQ"
   },
   "source": [
    "With $X$, $Y$ and $\\epsilon$ as previously defined, let's apply the model to the dataset : $Y = f(X) + \\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yeZpj-IkjrYk"
   },
   "source": [
    "As f(.) is a Gaussian Process, $f(X) \\sim N(0, K_\\gamma)$ where $K_\\gamma = (k_\\gamma (X_i, X_j))_{i,j \\in [\\![1,n]\\!]}$, and since $\\epsilon \\sim N(0, \\sigma ^2 I_n)$, it implies that $ (Y | X; \\gamma) \\sim N(0, K_\\gamma + \\sigma ^2 I_n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tSLF6m8fkt27"
   },
   "source": [
    "Consequently, the probability distribution is:  \n",
    "$$P(Y=y | X; \\gamma) = \\frac{1}{(2\\pi)^{n/2} det(K_\\gamma + \\sigma^2 I_n)^{1/2}}exp(-\\frac{1}{2} y^T (K_\\gamma + \\sigma^2 I_n)^{-1} y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c-7Qogrxk59t"
   },
   "source": [
    "The likelihood of the model, given the observed data, is defined as:  \n",
    "$$L(\\gamma) = P(Y=y | X; \\gamma)$$\n",
    "\n",
    "\n",
    "The aim of the training is to find the parameters $\\gamma$ which maximizes the likelihood function (Maximum Likelihood Estimation), which is the same as minimizing the negative log-likelihood. To unify with the neural network architecture used later, we chose to minimize the negative log-likelihood :  \n",
    "$$l(\\gamma) = - \\log (P(Y=y | X; \\gamma))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d4A7aOScmI3d"
   },
   "source": [
    "The resulting equation is thus:  \n",
    "$$l(\\gamma) = \\frac{1}{2} ( n\\log(2\\pi) + \\log det(K_\\gamma + \\sigma^2 I_n) + y^T (K_\\gamma + \\sigma^2 I_n)^{-1} y )$$\n",
    "\n",
    "To simplify this, we can rewrite $K_\\gamma + \\sigma ^2 I_n$ as $K_\\gamma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Som8Plcmuyr"
   },
   "source": [
    "Again, to unify this with the neural network approach (based on the chain rule), we need to compute the derivative of the negative log-likelihood with respect to $K_\\gamma$ :  \n",
    "$$\\frac {\\partial l}{\\partial K_\\gamma} = \\frac{1}{2} ( \\frac {\\partial n\\log(2\\pi)}{\\partial K_\\gamma} + \\frac {\\partial \\log det K_\\gamma }{\\partial K_\\gamma} + \\frac {\\partial y^T K_\\gamma^{-1} y )}{\\partial K_\\gamma})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7OX6rOcfu_BY"
   },
   "source": [
    "Pre-requisites :\n",
    "$$\\begin{equation}\n",
    "    \\begin{cases}\n",
    "      \\frac {\\partial \\log \\det A}{\\partial A} = A^{-1}\\\\\n",
    "      \\frac {\\partial A^{-1}}{\\partial A} = -(A^{-1}) (A^{-1})\\\\\n",
    "    \\end{cases}       \n",
    "\\end{equation}$$\n",
    "\n",
    "Consequently :\n",
    "$$\\begin{equation}\n",
    "    \\begin{cases}\n",
    "      \\frac {\\partial n\\log(2\\pi)}{\\partial K_\\gamma} = 0\\\\\n",
    "      \\frac {\\partial \\log det K_\\gamma}{\\partial K_\\gamma} = K_\\gamma ^{-1}\\\\\n",
    "      \\frac {\\partial y^T K_\\gamma^{-1} y}{\\partial K_\\gamma} = - K_\\gamma^{-1} y y^T K_\\gamma^{-1}\n",
    "    \\end{cases}       \n",
    "\\end{equation}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PROOF for $\\frac {\\partial y^T K_\\gamma^{-1} y}{\\partial K_\\gamma} = - K_\\gamma^{-1} y y^T K_\\gamma^{-1}$ :\n",
    ">\n",
    "> As $y^T K_\\gamma^{-1} y = \\sum_{i,j} y_i y_j (K_\\gamma^{-1})_{i,j}$, \n",
    "> $$\\frac {\\partial y^T K^{-1} y}{\\partial K_{k,l}} = \\sum_{i,j} y_i y_j (\\frac {\\partial K^{-1}} {\\partial K_{k,l}})_{i,j}$$ \n",
    ">\n",
    "> Given that $\\frac {\\partial K^{-1}} {\\partial K_{k,l}} =  - K^{-1} \\frac {\\partial K}{\\partial K_{k,l}} K^{-1}$  and that the derivative with respect to the coordinate (k,l) is equal to a (n,n) null matrix with a 1 at position (k,l),  it ends up with the following equation:\n",
    "> $$\\frac {\\partial K^{-1}} {\\partial K_{k,l}} =  - K^{-1} \\frac {\\partial K}{\\partial K_{k,l}} K^{-1} = - [(K^{-1}_{ik} K^{-1}_{lj})_{i,j}]$$\n",
    "> Consequently :\n",
    "$$(\\frac {\\partial K^{-1}} {\\partial K_{k,l}})_{i,j} = - K^{-1}_{ik} K^{-1}_{lj}$$\n",
    "Finally, the derivative equals : \n",
    "$$\\frac {\\partial y^T K^{-1} y}{\\partial K_{k,l}} = \\sum_{i,j} - y_i y_j K^{-1}_{ik} K^{-1}_{lj}.$$\n",
    ">\n",
    "> This may be written as :\n",
    "> $$\\frac {\\partial y^T K^{-1} y}{\\partial K_\\gamma}  = - K_\\gamma^{-1} y y^T K_\\gamma^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RcqrYPVtvmsA"
   },
   "source": [
    "Finally :  \n",
    "$$\\frac {\\partial l}{\\partial K_\\gamma} = \\frac{1}{2} (K_\\gamma ^{-1} - K_\\gamma^{-1} y y^T K_\\gamma^{-1})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nqLpimaiBgmG"
   },
   "source": [
    "In Deep Kernel Learning, we have $K_{\\gamma, w} = (k_\\gamma (h_w(X_i), h_w(X_j))_{i,j \\in [|1,n|]})$ where $h_w(.)$ represents the Neural Network.\n",
    "\n",
    "\n",
    "During the backpropagation, we use the chain rule to compute $\\frac {\\partial K_{\\gamma, w}}{\\partial \\gamma}$ and $\\frac {\\partial K_{\\gamma, w}}{\\partial w}$ in the following way :  \n",
    "\n",
    "$$\\begin{equation}\n",
    "    \\begin{cases}\n",
    "      \\frac {\\partial l}{\\partial \\gamma} = \\frac {\\partial l}{\\partial K_{\\gamma, w}} \\frac {\\partial K_{\\gamma, w}}{\\partial \\gamma}\\\\\n",
    "      \\frac {\\partial l}{\\partial w} = \\frac {\\partial l}{\\partial K_{\\gamma, w}} \\frac {\\partial K_{\\gamma, w}}{\\partial w}\n",
    "    \\end{cases}       \n",
    "\\end{equation}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RQn7bGWeepjn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Gaussian_Process.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
