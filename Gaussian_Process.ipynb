{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JmMtwMbe40JN"
   },
   "source": [
    "# Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mXFfsqJc421F"
   },
   "source": [
    "Dataset : $(X_i, Y_i)_{1⩽i⩽n}$ where $X_i  \\in \\mathbb{R}^d$ and $Y_i \\in \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kyUcoy3phok9"
   },
   "source": [
    "Regression problem : $\\forall i \\in \\{1, 2, ... n\\}, Y_i = f^*(X_i) + \\epsilon_i$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LIv0w-iwiu1W"
   },
   "source": [
    "Let's write the matrix form of this : $Y = f^*(X) + \\epsilon$  \n",
    "where $Y = [Y_1, Y_2, ... Y_n]^T \\in \\mathbb{R}^n$  \n",
    "and \n",
    "$X = \\begin{pmatrix}\n",
    "          - & X_1 & - \\\\\n",
    "          - & X_2 & - \\\\\n",
    "            & ... \\\\\n",
    "          - & X_n & -\n",
    "         \\end{pmatrix} \\in \\mathbb{R}^{n . d}$  \n",
    "and $\\epsilon = [\\epsilon_1, \\epsilon_2, ... \\epsilon_n]^T \\in \\mathbb{R}^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a-iQFDtchi9b"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tVO7SgFvgnBj"
   },
   "source": [
    "Model : $y = f(x) + \\epsilon$ where $f(.) \\sim GP(0, k_{\\gamma})$ and $\\epsilon \\sim N(0, \\sigma ^2 I_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mBrBd8kcEpI1"
   },
   "source": [
    "In general, the kernel used is the RBF kernel, defined as follow :  \n",
    "$k_\\gamma (z, z') = \\exp(-\\frac{1}{2} (z - z') / l^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "czDaG3sfhhcQ"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oUViGi3LhNL3"
   },
   "source": [
    "Gaussian Process Regression is a nonparametric model. We can do prediction directly by using the conditional gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9EeSQM8TjFqw"
   },
   "source": [
    "Given a dataset of observed outputs (X, Y), we want to predict the output of a test set $X_{test}$.  \n",
    "Let's define $Y_0 = [Y_{test}, Y]^T$, $X_0 = [X_{test}, X]^T$ and $\\epsilon_0 = [0, \\epsilon]^T$.  \n",
    "The model is then : $[Y_{test}, Y]^T = f([X_{test}, X]^T) + [0, \\epsilon]^T \\sim N(0, K_0 + \\sigma^2I_n)$  \n",
    "where $K_0 = \\begin{pmatrix}\n",
    "          K_{aa} & K_{ab} \\\\\n",
    "          K_{ba} & K_{bb} \\\\\n",
    "         \\end{pmatrix}$  \n",
    "We can then compute the conditional distribution $(Y_{test}|X_{test}, X, Y) \\sim N(m, D)$  \n",
    "By using the conditional Gaussian distribution formulas, we find the following results :  \n",
    "$m = K_{ab}K_{bb}^{-1}$  \n",
    "$D = (K_{aa} + \\sigma^2I_n) - K_{ab}K_{bb}^{-1}K_{ba}$  \n",
    "where $K_{aa} = (k_\\gamma(X_i, X_j)_{i,j})$  \n",
    "$K_{ab} = (k_\\gamma(X_i, X_{test, j})_{i,j})$  \n",
    "$K_{ba} = (k_\\gamma(X_{test, i}, X_j)_{i,j})$  \n",
    "$K_{bb} = (k_\\gamma(X_{test, i}, X_{test, j})_{i,j})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cGH2ZqsXjGWz"
   },
   "source": [
    "## Find the best kernel parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rS_5nm9TjRxQ"
   },
   "source": [
    "Let's apply the model to the dataset : $Y = f(X) + \\epsilon$  \n",
    "where $Y \\in \\mathbb{R}^n$, $X  \\in \\mathbb{R}^{n . d}$ and $\\epsilon \\in \\mathbb{R}^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yeZpj-IkjrYk"
   },
   "source": [
    "Because f(.) is a Gaussian Process, it means that $f(X) \\sim N(0, K_\\gamma)$ where $K_\\gamma = (k_\\gamma (X_i, X_j)_{1≤i, j≤n})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EER1DowWkZUH"
   },
   "source": [
    "Since $\\epsilon \\sim N(0, \\sigma ^2 I_n)$, it implies that $ (Y | X; \\gamma) \\sim N(0, K_\\gamma + \\sigma ^2 I_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tSLF6m8fkt27"
   },
   "source": [
    "Let's write the probability distribution :  \n",
    "$P(Y=y | X; \\gamma) = \\frac{1}{(2\\pi)^{n/2} det(K_\\gamma + \\sigma^2 I_n)^{1/2}}exp(-\\frac{1}{2} y^T (K_\\gamma + \\sigma^2 I_n)^{-1} y)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c-7Qogrxk59t"
   },
   "source": [
    "We define the likelihood of the model, given the observed data :  \n",
    "$L(\\gamma) = P(Y=y | X; \\gamma)$  \n",
    "We want to find the parameters $\\gamma$ which maximizes the likelihood function (Maximum Likelihood Estimation). Ordinarily, we prefer to maximize the log-likelihood : $\\log(L(\\gamma))$  \n",
    "To unify this to the neural network architecture that we will use later, we will instead minimizes the negative log-likelihood :  \n",
    "$l(\\gamma) = - \\log (P(Y=y | X; \\gamma))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d4A7aOScmI3d"
   },
   "source": [
    "Let's write the resulting equation :  \n",
    "$l(\\gamma) = \\frac{1}{2} ( n\\log(2\\pi) + \\log det(K_\\gamma + \\sigma^2 I_n) + y^T (K_\\gamma + \\sigma^2 I_n)^{-1} y )$  \n",
    "To simplify the notation, let's say that $K_\\gamma = K_\\gamma + \\sigma ^2 I_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Som8Plcmuyr"
   },
   "source": [
    "Again, to unify this with the neural network approach (based on the chain rule), we need to compute le derivative of the negative log-likelihood with respect to $K_\\gamma$ :  \n",
    "$\\frac {\\partial l}{\\partial K_\\gamma} = \\frac{1}{2} ( \\frac {\\partial n\\log(2\\pi)}{\\partial K_\\gamma} + \\frac {\\partial \\log det K_\\gamma }{\\partial K_\\gamma} + \\frac {\\partial y^T K_\\gamma^{-1} y )}{\\partial K_\\gamma})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7OX6rOcfu_BY"
   },
   "source": [
    "Pre-requisites :  \n",
    "$\\frac {\\partial \\log \\det A}{\\partial A} = A^{-1}$  \n",
    "$\\frac {\\partial A^{-1}}{\\partial A} = -(A^{-1}) (A^{-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_m-5JbspgPn"
   },
   "source": [
    "$\\frac {\\partial n\\log(2\\pi)}{\\partial K_\\gamma} = 0$  \n",
    "$\\frac {\\partial \\log det K_\\gamma}{\\partial K_\\gamma} = K_\\gamma ^{-1}$  \n",
    "$\\frac {\\partial y^T K_\\gamma^{-1} y}{\\partial K_\\gamma} = -y^T K_\\gamma^{-1} K_\\gamma^{-1} y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RcqrYPVtvmsA"
   },
   "source": [
    "Finally :  \n",
    "$\\frac {\\partial l}{\\partial K_\\gamma} = \\frac{1}{2} (K_\\gamma ^{-1} - y^T K_\\gamma^{-1} K_\\gamma^{-1} y)$  \n",
    "In the \"Deep Kernel Learning\" paper, the final result is : $\\frac {\\partial l}{\\partial K_\\gamma} = \\frac{1}{2} (K_\\gamma ^{-1} - K_\\gamma^{-1} y y^T K_\\gamma^{-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nqLpimaiBgmG"
   },
   "source": [
    "In the case of Deep Kernel Learning, we have $K_{\\gamma, w} = (k_\\gamma (h_w(X_i), h_w(X_j))_{1≤i, j≤n})$ where $h_w(.)$ represents the Neural Network.  \n",
    "During the backpropagation, we use the chain rule to compute $\\frac {\\partial K_{\\gamma, w}}{\\partial \\gamma}$ and $\\frac {\\partial K_{\\gamma, w}}{\\partial w}$ in the following way :  \n",
    "$\\frac {\\partial l}{\\partial \\gamma} = \\frac {\\partial l}{\\partial K_{\\gamma, w}} \\frac {\\partial K_{\\gamma, w}}{\\partial \\gamma}$  \n",
    "$\\frac {\\partial l}{\\partial w} = \\frac {\\partial l}{\\partial K_{\\gamma, w}} \\frac {\\partial K_{\\gamma, w}}{\\partial w}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RQn7bGWeepjn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Gaussian_Process.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
