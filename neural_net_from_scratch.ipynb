{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidFunction:\n",
    "    def __init__(self):\n",
    "        None\n",
    "    \n",
    "    def function(self, z):\n",
    "        return 1 / (1 + np.exp(z))\n",
    "    \n",
    "    def derivative(self, z):\n",
    "        return self.function(z) * (1 + self.function(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredCost:\n",
    "    def __init__(self):\n",
    "        None\n",
    "        \n",
    "    def compute(self, y_hat, y):\n",
    "        return np.dot((y_hat - y).T, y_hat - y)\n",
    "        \n",
    "    def final_derivative(self, a, y):\n",
    "        return a - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    \n",
    "    def __init__(self, layer_dims, activation, input_dim=None):\n",
    "        # Layer activation function\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Layer parameters\n",
    "        self.W = np.zeros((layer_dims, input_dim))\n",
    "        self.b = np.zeros((layer_dims, 1))\n",
    "        \n",
    "        # Cache\n",
    "        self.A_ = None\n",
    "        self.Z = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, A):\n",
    "        # Z[l] = W[l] * A[l-1] + b[l]\n",
    "        # A[l] = f(Z[l])\n",
    "        \n",
    "        self.A_ = A        \n",
    "        self.Z = np.dot(self.W, self.A_) + self.b\n",
    "        A = self.activation.function(self.Z)\n",
    "        \n",
    "        return A\n",
    "        \n",
    "    def backward(self, A_, dA=None, dZ=None):\n",
    "        if dZ == None:\n",
    "            dZ = self.W.dot(dA) * self.activation.derivative(self.Z)\n",
    "        dW = (1 / self.A_.shape[0]) * np.dot(dZ, self.A_.T)\n",
    "        db = (1 / self.A_.shape[0]) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA = np.dot(self.W.T, dZ)\n",
    "        return dA\n",
    "                \n",
    "        \n",
    "    def update_weights(self, learning_rate):\n",
    "        # W[l] = W[l] - alpha * dW[l]\n",
    "        # b[l] = b[l] - alpha * db[l]\n",
    "        \n",
    "        self.W = self.W - learning_rate * self.dW\n",
    "        self.b = self.b - learning_rate * self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.nb_layers = len(layers)\n",
    "    \n",
    "    def train(self, X, y, nb_epochs, learning_rate, cost):\n",
    "        for i in range(nb_epochs):     \n",
    "            \n",
    "            # Forward propagation\n",
    "            A = [X]\n",
    "            for l in range(self.nb_layers):\n",
    "                A.append(self.layers[l].forward(A[-1]))\n",
    "            \n",
    "            # Compute cost\n",
    "            y_hat = A[-1]\n",
    "            c = cost.compute(y_hat, y)\n",
    "\n",
    "            # Backward propagation\n",
    "            dZ = cost.final_derivative(A[-1], y)\n",
    "            dA = self.layers[self.nb_layers - 1].backward(A[self.nb_layers - 2])\n",
    "            for l in range(1, self.nb_layers):\n",
    "                dA = self.layers[self.nb_layers - l - 1].backward(A[self.nb_layers - l - 2], dA)\n",
    "\n",
    "            # Update weights\n",
    "            for l in range(self.nb_layers):\n",
    "                self.layers[l].update_weights(learning_rate)\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        a = X_test\n",
    "        for l in range(self.nb_layers):\n",
    "            a = self.layers[l].forward(a)\n",
    "        return a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.5 Predict-poverty",
   "language": "python",
   "name": "predict-poverty"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
