{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t3vvnS1b3vwC"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rOt2wkIe37Bc"
   },
   "outputs": [],
   "source": [
    "cd /content/drive/My\\ Drive/Cassiopé\\ Perso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sGOFQywr4UUJ"
   },
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SRWVhdNT9Hne"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import skimage.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvObsizOq4ls"
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "\n",
    "m=len([name for name in os.listdir('./cassiopee_data/') if os.path.isfile(name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FY-zAAccy2D3"
   },
   "outputs": [],
   "source": [
    "mean_bgr = [103.334, 107.8797, 107.4072]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gd8PnM9B9EkA"
   },
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "  # load image\n",
    "  img = skimage.io.imread(path)\n",
    "  for i in xrange(0, 3):\n",
    "    img[:, :, i] = img[:, :,i] - mean_bgr[i]\n",
    "  #img = img / 255.0\n",
    "  return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uVJm9tc-1P-T"
   },
   "source": [
    "On peut essayer de diviser les images par 255, mais je crois pas qu'il faut le faire pour VGG\n",
    "\n",
    "Il peut aussi être intelligent de soustraire mean_bgr à toutes les images et de les sauvegarder sous forme de .npy, plutot que faire l'opération à chaque étape de chargement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ahFUb0Ll3fiB"
   },
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CPccsCLa3mbF"
   },
   "outputs": [],
   "source": [
    "with open('cassiopee_data/images_donwloaded.csv', 'r') as csvFile:\n",
    "    reader = csv.reader(csvFile)\n",
    "    line_count=0\n",
    "    for row in reader:\n",
    "      if (line_count%10==0):\n",
    "        print(line_count)\n",
    "      np.save('data_npy/' + str(line_count-1) + '.npy',load_image(row[0]))\n",
    "      line_count+=1\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ry-pu2ik3jL-"
   },
   "source": [
    "### Labels loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YibEJA_4W4M"
   },
   "outputs": [],
   "source": [
    "with open('cassiopee_data/images_donwloaded.csv', 'r') as csvFile:\n",
    "    reader = csv.reader(csvFile)\n",
    "    Y=np.zeros((m,1))\n",
    "    line_count=0\n",
    "    for row in reader:\n",
    "      if (line_count%10==0):\n",
    "        print(line_count)\n",
    "      Y[line_count-1]=int(row[1])\n",
    "      line_count+=1\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XYyV7Awuemu"
   },
   "outputs": [],
   "source": [
    "print(Y[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ge7GBvilD-a0"
   },
   "source": [
    "### Classes restriction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "38Qgc2cDmfBr"
   },
   "outputs": [],
   "source": [
    "#0-63 intensities values to 0-2\n",
    "for i in range(Y.shape[0]):\n",
    "  if (Y[i][0]<=2):\n",
    "    Y[i][0]=0\n",
    "  elif (Y[i][0]<=34):\n",
    "    Y[i][0]=1\n",
    "  else:\n",
    "    Y[i][0]=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iRdxx81mtZ7k"
   },
   "outputs": [],
   "source": [
    "print(Y[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YNwcRDzBDrHt"
   },
   "source": [
    "#VGG-f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYiiqXJ4G3RT"
   },
   "source": [
    "##Download Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YAe_3ZeK34fo"
   },
   "outputs": [],
   "source": [
    "# execute if you haven't donwloaded the weights yet\n",
    "!wget http://www.vlfeat.org/matconvnet/models/imagenet-vgg-f.mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qcren7blG6Yy"
   },
   "source": [
    "##Model + Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dw0TGI8zEqPt"
   },
   "source": [
    "### Read .mat file and extract the weights of the conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ir9gMLdI4H67"
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat('imagenet-vgg-f.mat')\n",
    "dic={}\n",
    "print('Model Summary:')\n",
    "for i in range(21):\n",
    "  print(mat['layers'][0][i][0][0][0][0])\n",
    "  name=mat['layers'][0][i][0][0][0][0]\n",
    "  if (name[0:4]=='conv'):\n",
    "    weights=mat['layers'][0][i][0][0][2][0]\n",
    "    dic[name+'_weight']=weights[0]\n",
    "    dic[name+'_biais']=np.resize(weights[1],(weights[1].shape[0],))\n",
    "del mat\n",
    "del weights\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ADIXeIbL4gmI"
   },
   "outputs": [],
   "source": [
    "print(dic.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RLYwoICWGvaK"
   },
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "STnpbIfbf1UP"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OqkDzC8LQP3s"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (11,11),strides=4, activation='relu', input_shape=(400, 400, 3)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(256, (5,5), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(4096, (6,6), activation='relu', strides=6),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Conv2D(4096, (1,1), activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Conv2D(3, (1,1), activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.AveragePooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2oMRNO7at-Q0"
   },
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([dic['conv1_weight'],dic['conv1_biais']])\n",
    "model.layers[3].set_weights([dic['conv2_weight'],dic['conv2_biais']])\n",
    "model.layers[6].set_weights([dic['conv3_weight'],dic['conv3_biais']])\n",
    "model.layers[7].set_weights([dic['conv4_weight'],dic['conv4_biais']])\n",
    "model.layers[8].set_weights([dic['conv5_weight'],dic['conv5_biais']])\n",
    "del dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iFcIl-KNFKie"
   },
   "outputs": [],
   "source": [
    "assert(np.array_equal(model.layers[0].get_weights()[0],dic['conv1_weight']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qgWZpc0rhVD8"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iPXHsdVOUXOk"
   },
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sr0TnDGCsYX3"
   },
   "source": [
    "#### Creation of our DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cGCc_QMyUbt3"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(400,400,3), n_classes=3):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_classes = n_classes\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = np.load('./data_npy/' + ID + '.npy')\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lHf02xS4sQ6k"
   },
   "source": [
    "#### Split the dataset into train set and dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OBZHm46Panwx"
   },
   "outputs": [],
   "source": [
    "m_test=1000\n",
    "params = {'dim': (400,400,3), 'batch_size': 32, 'n_classes': 3}\n",
    "list_IDs=np.random.shuffle(np.arange(m))\n",
    "list_IDs_train=list_IDs[m_test:]\n",
    "list_IDs_test=list_IDs[0:m_test]\n",
    "del list_IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JXkWgngHajg8"
   },
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(list_IDs_train, labels, **params)\n",
    "validation_generator = DataGenerator(list_IDs_test, labels, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QCEYb5joIQ-M"
   },
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4YVGQIlIbcK"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_batch_end:\n",
    "    i+=1\n",
    "    (X, Y) = load_random_mini_batch(m,i, permutation)\n",
    "    \n",
    "  def on_epoch_end(self,epochs,logs={}):\n",
    "    permutation = list(np.random.permutation(m))    \n",
    "    i=0\n",
    "    (X, Y) = load_random_mini_batch(m,i, permutation)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2jbTGNQAvI3J"
   },
   "outputs": [],
   "source": [
    "# run for every training\n",
    "from time import time\n",
    "tensorboard=tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir='logs/{}'.format(time()), histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BeYWGO7bITRC"
   },
   "source": [
    "### Model compilation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3piW1NZJPQix"
   },
   "outputs": [],
   "source": [
    "adam=tf.keras.optimizers.Adam(lr=10**-6)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "# maybe we can try sgd optimizer with momentum=0.9, weight decay=5*10*-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ontkobQHaQgW"
   },
   "outputs": [],
   "source": [
    "model.fit_generator(generator=training_generator, validation_data=validation_generator, epochs=10, callbacks=[tensorboard])\n",
    "#model.fit_generator(generator=training_generator, validation_data=validation_generator, epochs=10, use_multiprocessing=True, workers=6, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QwhozajCvqRE"
   },
   "source": [
    "**1. Run in your terminal: tensorboard --logdir=logs/**\n",
    "\n",
    "\n",
    "**2. Open the link, in your browser**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "datVKGa5Ge73"
   },
   "source": [
    "### Save and load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZuLhzXxtsh2v"
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QL_2ZzfZP6G5"
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FT4oMH9FQFka"
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P13QwZV8Gl1l"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPY2HuTfMPaf"
   },
   "outputs": [],
   "source": [
    "model.evaluate_generator(validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zhqR7z1pGodx"
   },
   "source": [
    "### Model cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VupD2U1Enwmx"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cvBhvyXB7fxl"
   },
   "source": [
    "#VGG 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "63TdshuNRJPG"
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3wr5kO4d4Neb"
   },
   "outputs": [],
   "source": [
    "model = VGG16(weights='imagenet',include_top=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PCxpZxEL8fsf"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jEj1JPbw4XyZ"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Convolution2D(4096, (7,7), activation='relu'))\n",
    "model.add(tf.keras.layers.Convolution2D(4096, (1,1), activation='relu'))\n",
    "model.add(tf.keras.layers.Convolution2D(3, (1,1), activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o85zOeBM7rBM"
   },
   "source": [
    "#Bash command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ngMXhuz7N4y"
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "64HSQ_Cz4a0m"
   },
   "outputs": [],
   "source": [
    "!ls cassiopee_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zu0laDII4eE3"
   },
   "outputs": [],
   "source": [
    "ls -l cassiopee_data/image-nightlight_dataset | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NAZn-3TN6TBN"
   },
   "outputs": [],
   "source": [
    "!find cassiopee_data/image-nightlight_dataset/3674.0.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r70Gm8Ak7Tdu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "datVKGa5Ge73",
    "P13QwZV8Gl1l",
    "cvBhvyXB7fxl",
    "o85zOeBM7rBM"
   ],
   "name": "keras-vgg-f.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
