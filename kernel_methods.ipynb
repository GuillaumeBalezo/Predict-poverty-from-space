{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BglU2CgMyxwt"
   },
   "source": [
    "# Kernel methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AgY45NVlzA-w"
   },
   "source": [
    "The regularized empirical risk minimization problem is : $\\hat f = argmin_{f \\in F} \\frac{1}{n} \\sum_{i = 1}^n L(y_i, f(x_i)) + \\lambda \\Omega (f)$\n",
    "\n",
    "A simple example, linear models : $X = R^p$, $F = \\{ f_w : x \\mapsto w^T x | w \\in R^p \\}$, $\\Omega (f_w) = ||w||^2_2$\n",
    "\n",
    "By choosing carefully the loss function, we can create several well-known models :\n",
    "- ridge regression : $L(y_i, w^T x_i) = \\frac{1}{2} (y_i - w^T x_i)^2$\n",
    "- linear SVM : $L(y_i, w^T x_i) = max(0, 1 - y_i w^T x_i)$\n",
    "- logistic regression : $L(y_i, w^T x_i) = log(1 + e^{-y_i w^T x_i})$\n",
    "\n",
    "Unfortunately, linear models often perform poorly unless the problem features are well-engineered or the problem is very simple.\n",
    "\n",
    "To solve this problem, we need to change the functional space F :  \n",
    "1. By choosing F as a deep learning space  \n",
    "2. By choosing F as a RKHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G3jC9FPRy0EF"
   },
   "source": [
    "## I) Kernels and RKHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcJQ0J28y4qQ"
   },
   "source": [
    "### 1) Positive definite kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d5xQyDMGzADq"
   },
   "source": [
    "The kernel method is based on pairwise comparisons between data points. We define a \"comparison function\" $K : X^2 \\to R$ and represent a set of n data points $S = { x_1, ... x_n}$ by the n x n matrix $K = [K(x_i, x_j)]_{1 <= i,j <= n}$. However, we will restrict ourselves to a particular class of pairwise comparison functions : positive definite kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wkEvpopM1vaX"
   },
   "source": [
    "**Definition :** A **positive definite kernel** on the set X is a function $K : X^2 \\to R$ that is symmetric and which satisfies : $$\\forall n \\in N, \\forall x_1, ... x_n \\in X^n, \\forall a_1, ... a_n \\in R^n, \\sum_i \\sum_j a_i a_j K(x_i, x_j) \\geq 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CPNBetLv2d6k"
   },
   "source": [
    "Equivalently, a kernel K is pd if and only if for any set of n data points, the associated matrix K is symmetric and positive semidefinite.\n",
    "\n",
    "$\\forall n \\in N, \\forall x_1, ... x_n \\in X^n, K = [K(x_i, x_j)]_{1 <= i,j <= n}$ is symmetric and positive semidefinite, ie :\n",
    "$$K^T = K$$\n",
    "$$\\forall u \\in R^n, u^T K u \\geq 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yDWwQi-52seF"
   },
   "source": [
    "> __Example : linear kernel__  \n",
    "$X = R^d$\n",
    "$$K : X^2 \\to R \\\\\n",
    "(x, y) \\mapsto \\langle x, y \\rangle$$\n",
    "K is symmetric by definition of the inner product in $R^d$ and verifies : $\\sum_i \\sum_j a_i a_j K(x_i, x_j) = \\sum_i \\sum_j a_i a_j \\langle x_i, x_j \\rangle = \\langle \\sum_i a_i x_i, \\sum_j a_j x_j \\rangle = || \\sum_i a_i x_i ||^2 \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9rhaSC-y5YFU"
   },
   "source": [
    "**Lemma :** $\\phi : X \\to R^d$. If \n",
    "$$K : X^2 \\to R \\\\\n",
    "(x, y) \\mapsto \\langle \\phi(x), \\phi(y) \\rangle$$\n",
    "Then K is a pd kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7s6ftFP56g3"
   },
   "source": [
    "> **Proof :** K is symmetric by definition of the inner product in $R^d$ and verifies : $\\sum_i \\sum_j a_i a_j K(x_i, x_j) = \\sum_i \\sum_j a_i a_j \\langle \\phi(x_i), \\phi(x_j) \\rangle = \\langle \\sum_i a_i \\phi(x_i), \\sum_j a_j \\phi(x_j) \\rangle = || \\sum_i a_i \\phi(x_i) ||^2 \\geq 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8--Ibkcs6n4n"
   },
   "source": [
    "> __Example : polynomial kernel__  \n",
    "$X = R^2$\n",
    "$$K(x, y) = \\langle \\phi(x), \\phi(y) \\rangle$$\n",
    "where $\\phi : R^2 \\to R^3 \\\\ x = (x_1, x_2) \\mapsto (x_1^2, \\sqrt2x_1x_2, x_2^2)$  \n",
    "Then K is a pd kernel and we can show that :  \n",
    "$$K(x, y) = x_1^2 y_1^2 + 2x_1x_2y_1y_2 + x_2^2y_2^2 = (x_1y_1 + x_2y_2)^2 = \\langle x, y \\rangle ^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DHVnrsgE79Z-"
   },
   "source": [
    "The converse of the previous lemma is a fundamental theorem in kernel methods : it shows that any pd kernels can be considered as an inner product in a Hilbert space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Fu00Dkw79cu"
   },
   "source": [
    "**Theorem : K is a pd kernel if and only if there exists a Hilbert space H and a mapping $\\phi : X \\to H$ such that \n",
    "$$\\forall x, y \\in H^2, K(x, y) = \\langle \\phi(x), \\phi(y) \\rangle$$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e9uwvkhB9Bzj"
   },
   "source": [
    "> **Proof :** finite case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ub_0WD399ESz"
   },
   "source": [
    "> **Proofs :**\n",
    "- if X is a compact and K continuous : Mercer's proof\n",
    "- if X is countable : Kolmogorov's proof\n",
    "- for the general case : Aronszajn's proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EM65WrM49bCN"
   },
   "source": [
    "We will go through the proof of the general case by introducing the concept of Reproducing Kernel Hilbert Space (RKHS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nTTT0g9Z9fcE"
   },
   "source": [
    "### 2) Reproducing Kernel Hilbert Space (RKHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6M32Wi459mPm"
   },
   "source": [
    "**Definition :**  Let X be a set, $H \\subset R^X$ a class of functions $X \\to R$ forming a (real) Hilbert space (with inner product $\\langle ., . \\rangle$).  \n",
    "The function $K : X^2 \\to R$ is called a **reproducing kernel** of the Hilbert space H if and only if :\n",
    "- H contains all functions of the form : $\\forall x \\in X, K_x : t \\to K(x, t)$\n",
    "- For all $x \\in X$ and for all $f \\in H$, the reproducing property holds : $f(x) = \\langle f, K_x \\rangle$\n",
    "\n",
    "If a reproducing kernel of H exists, then H is called a **reproducing kernel Hilbert space** (RKHS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFcO1etNoVl2"
   },
   "source": [
    "**Theorem : The Hilbert space H is a RKHS if and only if for all $x \\in X$ $$F : H \\to R \\\\ f \\mapsto f(x)$$ is continuous.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jcyplKNbpEH3"
   },
   "source": [
    "> **Proof :**  \n",
    "($\\Rightarrow$) H is a RKHS. We wonder if $$F : H \\to R \\\\ f \\mapsto f(x)$$ is continuous  \n",
    "We can show that F is L-smooth. Because H is a RKHS, there exists a reproducing kernel K and for any $x \\in X$ and any $f, g \\in H^2$ :\n",
    "$$ || F(f) - F(g) || = | f(x) - g(x) | = \n",
    "| \\langle f - g, K_x \\rangle | \\\\\\leq || f - g ||_H . || K_x ||_H \\leq || f - g ||_H . \\sqrt \\langle K_x, K_x \\rangle \\\\\\leq || f - g ||_H . \\sqrt{K(x, x)}$$\n",
    "Hence, F is L-smooth (with $L = \\sqrt{K(x, x)}$) and thus continuous.  \n",
    "($\\Leftarrow$) F is continuous. We want to show that H is a RKHS, i.e. there exists a reproducing kernel K for H  \n",
    "By using the **Riesz representation theorem** (an important property of Hilbert spaces) : if H is an Hilbert space then any continuous linear form f on H can be written as the inner product such that $f(.) = \\langle ., y \\rangle$ where $y \\in H$ is unique  \n",
    "Yet, F is a continuous linear form on H where the elements of H are functions. Hence :\n",
    "$$ \\forall x \\in X, \\exists ! g_x \\in H, F(f) = f(x) = \\langle f, g_x \\rangle$$\n",
    "Finally, the function $K(x, y) = g_x (y)$ is a rk for H because it holds the reproducing property and $\\forall x \\in X, g_x \\in H$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ipr7WBjzxtJO"
   },
   "source": [
    "**Corollary :** Convergence in a RKHS implies pointwise convergence, i.e. if $(f_n)_{n \\in N}$ converges to $f$ in H, then, for any $x \\in X$, $(f_n(x))_{n \\in N}$ converges to $f(x)$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5QnzWCDo_-uK"
   },
   "source": [
    "The following theorem proves the equivalence between a positive definite kernel and a reproducing kernel and will allow us to prove the fundamental theorem which says that any positive definite kernel can be represented as an inner product in some Hilbert space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VPRBvqkeyM3X"
   },
   "source": [
    "**Theorem : A function $K : X^2 \\to R$ is a positive definite kernel if and only if it is a reproducing kernel for some Hilbert space H.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nXq69b8-AZI_"
   },
   "source": [
    "> **Proof :**  \n",
    ">($\\Leftarrow$) If K is a reproducing kernel for a Hilbert space H, then it can be expressed as :\n",
    "$$K(x, y) = K_x(y) = \\langle K_x, K_y \\rangle$$\n",
    "Hence, K is symmetric by definition of the inner product in H and\n",
    "$$\\forall x_1, ... x_n \\in X^n, \\forall a_1, ... a_n \\in R, \n",
    "\\\\\\sum_i \\sum_j a_i a_j K(x_i, x_j) \n",
    "\\\\= \\langle \\sum_i a_i K_{x_i}, \\sum_j a_j K_{x_j} \\rangle \n",
    "\\\\= || \\sum_i a_i K_{x_i} || ^2 \\geq 0$$\n",
    "Then, K is a positive definite kernel.  \n",
    ">($\\Rightarrow$) K is a positive definite kernel. We need to create a RKHS H for which K will be the reproducing kernel.  \n",
    "Let $H_0$ be the vector subspace of $R^X$ spanned by the functions $(K_x)_{x \\in X}$ :\n",
    "$$H_0 = vect((K(x, .))_{x \\in X})$$   \n",
    "We want to define an inner product such that $H_0$ is an pre-Hilbert space.  \n",
    "...  \n",
    "But we don't have the completeness, so $H_0$ is not an Hilbert space. We then extends $H_0$ by creating $H \\subset R^X$ to be the set of functions $f : X \\to R$ which are pointwise limits of Cauchy sequences (of functions) of $H_0$.  \n",
    "We can prove that :\n",
    "- $H_0 \\subset H$\n",
    "- the application defined as the limit of inner products of Cauchy sequences (of functions) of $H_0$ is a well-defined inner product of $H$\n",
    "- by construction, we can show that $H$ is complete\n",
    "- finally, we prove that K is a reproducing kernel for H (in particular, the reproducing property holds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-K2uBnSkD_YR"
   },
   "source": [
    "Finally, we can deduce easily the Aronszajn's theorem (the general case of Mercer's theorem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T5zL7nTKEOGS"
   },
   "source": [
    "**Theorem : K is a positive definite kernel on the set X if and only if there exists a Hilbert space H and a mapping $\\phi : X \\to H$ such that, for any $x, y \\in X^2$ :\n",
    "$$ K(x, y) = \\langle \\phi(x), \\phi(y) \\rangle _H $$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0B2EOjNFR3d"
   },
   "source": [
    "> **Proof :**  \n",
    "($\\Leftarrow$) Already proved.  \n",
    "($\\Rightarrow$) We proved that if K is a positive definite kernel then there exists a Hilbert space H such that K is a reproducing kernel for H. If we define the mapping $\\phi : X \\to H$ by :\n",
    "$$ \\forall x \\in X, \\phi(x) = K_x = K(x, .)$$\n",
    "Then, by reproducing property, we have :\n",
    "$$ \\forall (x, y) \\in X^2, \\langle \\phi(x), \\phi(y) \\rangle _H = \\langle K_x, K_y \\rangle _H = K_x(y) = K(x, y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t0w5VH-tGnJK"
   },
   "source": [
    "### 3) My first kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wCD7DOa7Gx6I"
   },
   "source": [
    "Let's see some kernel examples and discover the RKHS associated to these kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8quFrTBVG4H0"
   },
   "source": [
    "### 4) Smoothness functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CW9UYgZAG7Ju"
   },
   "source": [
    "Here we demonstrate that there is a natural way to regularize functions in a RKHS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uxafe_xjHA1L"
   },
   "source": [
    "### 5) The kernel trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1mKHEnv5HDPu"
   },
   "source": [
    "We can show that kernel methods allow us to create efficient nonlinear methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mje2M3awHK4S"
   },
   "source": [
    "## II) Kernel methods : supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ku8FPgFaHPSO"
   },
   "source": [
    "### 1) The representer theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cjxbh0jwHbN1"
   },
   "source": [
    "The representer theorem says that the solution to a regularized empirical risk minimization problem in a RKHS lives in the vector subspace spanned by the kernel functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G8BFmhdNH4dg"
   },
   "source": [
    "### 2) Kernel ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wc5ellNEH6f6"
   },
   "source": [
    "Kernel ridge regression is a useful extension of ridge regression by searching a solution function in a RKHS. This extension is allowed thanks to the kernel trick. By the representer theorem, the solution can be easily written and compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "enRrhKWzImNb"
   },
   "source": [
    "### 3) Classification with empirical risk minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xu68BAF0IqIz"
   },
   "source": [
    "optimization in RKHS can be seen as an unconstrained and convex optimization problem in $R^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2FS03DYZI2H3"
   },
   "source": [
    "### 4) A (tiny) bit of learning theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ttl-MC2CI5rz"
   },
   "source": [
    "definitions\n",
    "\n",
    "large-margin classifiers -> empirical risk minimization\n",
    "\n",
    "capacity and Rademacher complexity\n",
    "\n",
    "basic learning bounds\n",
    "\n",
    "ERM in RKHS balls, constrained optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uzAzg4vLJP5O"
   },
   "source": [
    "### 5) Foundations of constrained optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XLG657PKJSch"
   },
   "source": [
    "convexity, Lagrangian, duality, KKT conditions, Slater's condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "49wGc5wqJS9m"
   },
   "source": [
    "### 6) Support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iRFnvVF6Jgg6"
   },
   "source": [
    "## III) Kernel methods : unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cC6sO6g5JS_z"
   },
   "source": [
    "## IV) The kernel jungle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gpSYo55XJTCT"
   },
   "source": [
    "## V) Open problems and research topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgUs-Ej5JTEg"
   },
   "source": [
    "Bibliographie :\n",
    "\n",
    "http://lear.inrialpes.fr/people/mairal/teaching/2015-2016/MVA/fichiers/mva_slides.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wK4jbGs887Ly"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "kernel_methods.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
